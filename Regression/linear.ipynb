import numpy as np
import pandas as pd

# ---------------- Load Dataset ----------------
data = pd.read_csv("diabetes_dataset.csv")
# Normalize feature for faster convergence
X = (X - X.mean()) / X.std()

# Add bias term (intercept)
X = np.c_[np.ones((X.shape[0], 1)), X]   # shape: (n_samples, 2)

# ---------------- Sigmoid Function ----------------
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# ---------------- Parameters ----------------
w = np.zeros(X.shape[1])   # weights [bias, coefficient]
L = 0.01                   # learning rate
epochs = 1000
n = len(X)

# ---------------- Gradient Descent ----------------
for i in range(epochs):
    z = np.dot(X, w)            # linear combination
    y_pred = sigmoid(z)         # apply sigmoid
    
    # Gradient of loss function (Binary Cross-Entropy)
    gradient = (1/n) * np.dot(X.T, (y_pred - y))
    
    # Update weights
    w -= L * gradient
    
    # Print loss occasionally
    if i % 100 == 0:
        loss = -np.mean(y*np.log(y_pred+1e-9) + (1-y)*np.log(1-y_pred+1e-9))
        print(f"Epoch {i}: Loss={loss:.4f}")

print("\nFinal Weights:", w)

# ---------------- Predictions ----------------
y_pred_class = (sigmoid(np.dot(X, w)) >= 0.5).astype(int)
accuracy = np.mean(y_pred_class == y)
print("Accuracy:", accuracy)
